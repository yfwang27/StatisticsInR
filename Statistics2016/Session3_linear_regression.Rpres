Session3: Linear Regression
========================================================
author: MRC Clinical Sciences Centre (http://mrccsc.github.io/)
date: 12/July/2016
width: 1440
height: 1100
autosize: true
font-import: <link href='http://fonts.googleapis.com/css?family=Slabo+27px' rel='stylesheet' type='text/css'>
font-family: 'Slabo 27px', serif;
css:style.css

Outline
========================================================
- correlation

- linear regression


Dataset - use the "iris" data (1/3)
========================================================
We have used the "iris" data for the [Intermediate R course]. We are going to work on this data again for this session.

Dataset - use the "iris" data (2/3)
========================================================
Some basic checks
```{r}
class(iris)
str(iris)
```
***
```{r}
head(iris)
```


Dataset - use the "iris" data (3/3)
========================================================
[*intermediate R; add a link to the intermediate R]
```{r}
# library("dplyr")
# tbl_df()

```
Correlation
=========================================================

A common task in statistical analysis is to investigate the relationship between pairs of numeric vectors.

This can be done by identifying the correlation between numeric vectors using the **cor()** function in R.

In this example we use cor() to identify the Pearson correlation between two variables.  The **method** argument may be set to make use of different correlation methods.

- Perfectly posively correlated vectors will return 1
- Perfectly negatively correlated vectors will return -1
- Vectors showing no or little correlation will be close to 0.

Correlation (1/)
========================================================
- pearson

- spearman

Correlation between vectors
=========================================================

```{r}
x <- rnorm(100,10,2)
z <- rnorm(100,10,2)
y <- x
cor(x,y) #
cor(x,-y)
cor(x,z)
```
***
```{r,echo=F}
par(mfrow=c(3,1))
plot(x,y) #
plot(x,-y)
plot(x,z)

par(mfrow=c(1,1))
```


Correlation over a matrix
=========================================================
left: 70%
Often we wish to apply correlation analysis to all columns or rows in a matrix in a pair-wise manner. To do this in R, we can simply pass the **cor()** function a single argument of the numeric matrix of interest. The **cor()** function will then perform all pair-wise correlations between columns.
```{r}
minRep <- rbind(cbind(matrix(rnorm(12,4),ncol=3,byrow = T),
                      matrix(c(rnorm(9,4),rnorm(3,8)),ncol=3,byrow = T)),
                cbind(matrix(rnorm(12,10),ncol=3,byrow = T),
                      matrix(c(rnorm(6,3),rnorm(6,10)),ncol=3,byrow = T)))
colnames(minRep) <- paste0(c("Sample_"),
                      1:5,".",sort(rep(c("hi","low"),3)))
minRepdf <- data.frame(Gene_Name=paste("Gene",letters[1:8],sep="_"),minRep)

#kable(minRepdf)
```


Correlation over a matrix
=========================================================
```{r,echo=F}
#kbale(minRep[1:2,])
```

```{r,eval=F}
cor(minRep)[1:2,2:5]
```
```{r,echo=F}
cor(minRep)[1:2,2:5,drop=F]
```

Visualising correlation
=========================================================

```{r,eval=F,echo=T,fig.width=4,fig.height=3,dpi=300,out.width="1920px",height="1080px"}
image(cor(minRep),axes=F)
axis(1,at=seq(0,1,length.out=6), colnames(minRep))
axis(2,at=seq(0,1,length.out=6), colnames(minRep))
```
***
```{r,eval=T,echo=F,fig.width=4,fig.height=3,dpi=300,out.width="1920px",height="1080px"}
image(cor(minRep),axes=F)
axis(1,at=seq(0,1,length.out=6),gsub(".*_","",colnames(minRep)),las=2)
axis(2,at=seq(0,1,length.out=6),rev(gsub(".*_","",colnames(minRep))),las=2)
```

Correlation (2/)
========================================================
```{r}
cor(iris[,1:4])
```
***
```{r,eval=T,echo=F,fig.width=4,fig.height=4,dpi=300,out.width="820px",height="820px"}
image(cor(iris[,1:4]),axes=F)
axis(1,at=seq(0,1,length.out=4),gsub(".*_","",colnames(iris[,1:4])),las=2)
axis(2,at=seq(0,1,length.out=4),rev(gsub(".*_","",colnames(iris[,1:4]))),las=2)
```

Correlation (3/)
========================================================
```{r}
pairs(iris[,1:4])
```

Correlation (4/)
========================================================
[probably find an example that requires the spearman method]
```{r}
cor(iris[,1:4],method="spearman")
```

Regression and linear models (1/)
=========================================================

We have seen how we can find the correlation between two sets of variables using cor() function.

R also provides a comprehensive set of tools for regression analysis including the well used linear modeling function lm()

To fit a linear regression we use a similar set of arguments as passed to the t-test fuction in the previous slide.

Regression and linear models (/)
=========================================================
Using the Petal.Width from iris data as example

We could like to use the current information to predict the width of a petal from Iris.versicolor
![alt text](imgs/Iris_versicolor.jpg)
***
```{r}
iris_versi<-iris[iris$Species=="versicolor",]

str(iris_versi)
```


Regression and linear models (/)
=========================================================
Try to use the mean of total Petal.Length first
```{r}
head(iris_versi[,c("Petal.Length",
                   "Species")])
mean(iris_versi$Petal.Length)
```
***
```{r}
plot(iris_versi$Petal.Length, ylab="Petal Length of Iris.versicolor")
abline(h=mean(iris_versi$Petal.Length),
       col="forestgreen",lwd=3)
```


Regression and linear models (/)
=========================================================
Try to use the mean of total Petal.Length first

```{r,echo=F}
diff_df<-iris_versi$Petal.Length-mean(iris_versi$Petal.Length)
plot(iris_versi$Petal.Length,ylab="Petal Length of Iris.versicolor")
abline(h=mean(iris_versi$Petal.Length),
       col="forestgreen",lwd=3)

segments(x0=c(1:nrow(iris_versi)),y0=iris_versi$Petal.Length,
         x1=c(1:nrow(iris_versi)),y1=mean(iris_versi$Petal.Length),col="pink")
```
***
In this case, the expected values is  $$ mean  = \bar{y} $$
- residuals (Error)
$$
  \begin{aligned}

  Error_i & = y_i - \bar{y}
  \\ \\
  \end{aligned}
$$
- square of the residuals
- sum of the square of the residuals (SSE)


Regression and linear models (/)
=========================================================
Zoom in [just see first 4 data points]

```{r,echo=F}
diff_df<-iris_versi$Petal.Length-mean(iris_versi$Petal.Length)
plot(iris_versi$Petal.Length[c(1:4)],xlim=c(0,5.5), ylim=c(2,6.5),
     ylab="Petal Length of first 4 data points (y)", xlab="x")
abline(h=mean(iris_versi$Petal.Length),
       col="forestgreen",lwd=3)

rect(xleft=c(1:4),ybottom=mean(iris_versi$Petal.Length),
     xright=c(1:4)+abs(diff_df[c(1:4)]),
     ytop=mean(iris_versi$Petal.Length)+diff_df[c(1:4)],
     density=30,col = "blue")

segments(x0=c(1:4),y0=iris_versi$Petal.Length[c(1:4)],
         x1=c(1:4),y1=mean(iris_versi$Petal.Length),col="pink",lwd=3)
```
***
In this case, the expected values is  $$ mean  = \bar{y} $$
- residuals (Error)
$$
  \begin{aligned}
  \\
  Error_i & = y_i - \bar{y}
  \end{aligned}
$$
- square of the residuals
$$
  \begin{aligned}
  Error_i^2  = (y_i - \bar{y})^2
  \end{aligned}
$$
- sum of the square of the residuals (SSE)
$$
  \begin{aligned}

  SSE  = \sum_{i=1}^{n}(y_i-\bar{y})^2
  \end{aligned}
$$


Regression and linear models (/)
=========================================================
Use the "iris_versi" Petal.Width to predict Petal.Length

```{r,echo=F}
plot(iris_versi$Petal.Width,iris_versi$Petal.Length,
     xlab="Petal Width of Iris.versicolor (x)",
     ylab="Petal Length of Iris.versicolor (y)")
abline(lm(iris_versi$Petal.Length~iris_versi$Petal.Width,data=iris_versi),
       col="blueviolet",lwd=3, lty=3)
title("Scatter plot of Iris.versicolor Petal.Width vs Petal.Length")

```
***
$$
  x = \text{independent or explanatory variable}
\\
  y = \text{dependent variable or }f(x)

$$

**$$f(x)  = b_0 + b_1x$$**

$$\begin{aligned}
  b_0 = intercept
  \\
  b_1 = slope
\end{aligned}$$



Regression and linear models (/)
=========================================================
The lm() function fits a linear regression to your data and provides useful information on the generated fit.

In the example below we fit a linear model using  lm() on the iris_versi dataset with Petal.Length (Y) as the dependent variable and Petal.Width (X) as the explanatory variable.
```{r}
lmResult<-lm(formula = Petal.Length ~ Petal.Width, data = iris_versi)
lmResult
```


Regression and linear models (/)
=========================================================
```{r}
summary(lmResult)
```

Regression and linear models (/)
=========================================================
```{r}
lmResult$coefficients
```
From the $coefficients of object lmResult, we know the equation for the best fit is

**Y = 1.781275 + 1.869325 *X**

We can add the line of best fit using **abline()**
***
```{r,echo=F}
plot(iris_versi$Petal.Width,iris_versi$Petal.Length,xlim=c(0,1.8),ylim=c(0,5),
     xlab="Petal Width of Iris.versicolor (x)",
     ylab="Petal Length of Iris.versicolor (y)")
abline(lm(iris_versi$Petal.Length~iris_versi$Petal.Width,data=iris_versi),
       col="blueviolet",lwd=3, lty=3)
title("Scatter plot of Iris.versicolor Petal.Width vs Petal.Length")
```


Statistics (22/26) - Plotting line of best fit.
=========================================================

From the previous slides we now know the formula for the line.

**Y = 7.001 + 1.972*X**

We can add the line of best fit using **abline()**

```{r,echo=T,prompt=T}
plot(Y~X,data=lmExample,main="Line of best fit with lm()",
     xlim=c(0,100),ylim=c(0,200))
abline(lmResult,col="red",lty=3,lwd=3)
```

Statistics (23/26) - Interpreting output of lm()
=========================================================
As we have seen, printing the model result provides the intercept and slope of line.

To get some more information on the model we can use the summary() function

```{r,prompt=T}
summary(lmResult)
```


Statistics (24/26) - Residuals
=========================================================

```{r,prompt=T,echo=F}
summary(lmResult)
```

***
The **residuals** are the difference between the predicted and actual values.
To retrieve the residuals we can access the slot or use the resid() function.

```{r,prompt=T,echo=T}
summary(resid(lmResult))
summary(lmResult$residual)
```
Ideally you would want your residuals to be normally distributed around 0.

Statistics (25/26) - R-squared
=========================================================

```{r,prompt=T,echo=F}
summary(lmResult)
```

The **R-squared** value represents the proportion of variability in the response variable that is explained by the explanatory variable.

A high **R-squared** here indicates that the line fits closely to the data.

Statistics (26/26) - F-statistics.
=========================================================

```{r,prompt=T}
summary(lmResult)
```

The results from linear models also provides a measure of significance for a variable not being relevant.

Statistics (Extra) - A fit line
=========================================================

![alt text](imgs/fittedline.png)

Statistics (Extra) - Calculating R-squared
=========================================================

![alt text](imgs/rsquared.png)

Statistics (Extra) - Calculating R-squared
=========================================================

```{r,prompt=F}
SSE <- sum(resid(lmResult)^2)
TSS <- sum((lmExample$Y - mean(lmExample$Y))^2)
1- SSE/TSS
summary(lmResult)$r.squared
```

Statistics (Extra) - Calculating F-stat
=========================================================

![alt text](imgs/fstatistic.png)

Statistics (Extra) - Calculating F-stat
=========================================================

```{r,prompt=F}
MSE <- mean(lmResult$residuals^2)
RSS <- sum((predict(lmResult) - mean(lmExample$Y))^2)

summary(lmResult)$fstatistic

```



Regression and linear models
=========================================================

```{r,echo=T,prompt=T}
lmExample <- read.table("data/lmExample.txt",h=T,sep="\t")
```
```{r,echo=T,prompt=T}
lmResult <- lm(Y~X,data=lmExample)
plot(Y~X,data=lmExample,main="Line of best fit with lm()",
     xlim=c(0,150),ylim=c(0,200))
abline(lmResult,col="red",lty=3,lwd=3)
```
